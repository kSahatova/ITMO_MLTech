{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLTech_Lab5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6fwuQxsaZ9xB2fXo0Jzwj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kSahatova/ITMO_MLTech/blob/main/Lab5/MLTech_Lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lidNqZPMhMoe"
      },
      "source": [
        "## Task Assignment\r\n",
        "\r\n",
        "1.\tDownload Alice in Wonderland by Lewis Carroll from Project Gutenberg's website http://www.gutenberg.org/files/11/11-0.txt\r\n",
        "2.\tPerform any necessary preprocessing on the text, including converting to lower case, removing stop words, numbers / non-alphabetic characters, lemmatization.\r\n",
        "3.\tFind Top 10 most important (for example, in terms of TF-IDF metric) words from each chapter in the text (not \"Alice\"); how would you name each chapter according to the identified tokens?\r\n",
        "4.\tFind the Top 10 most used verbs in sentences with Alice. What does Alice do most often?\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQtunD60hLY0",
        "outputId": "769399f1-8bec-45cb-a630-90619fadff73"
      },
      "source": [
        "import requests\r\n",
        "import re\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import nltk\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "\r\n",
        "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.tag import pos_tag\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQQ4-brgPtH-"
      },
      "source": [
        "def clean_text(text):\r\n",
        "    # remove several non-alphabetic characters\r\n",
        "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9fâ_]', ' ', text)\r\n",
        "    # remove numbers\r\n",
        "    text = re.sub(r'[0-9]', ' ', text)\r\n",
        "    # convert text to lower case\r\n",
        "    text = text.lower()\r\n",
        "    text = text.split(\" \")\r\n",
        "    # remove stop words\r\n",
        "    text = [word for word in text if not word in stop_words if word != 'alice']\r\n",
        "    # lemmatization\r\n",
        "    text = [lemmatizer.lemmatize(token) for token in text]\r\n",
        "    # remove other non-alphabetic characters\r\n",
        "    text = [token for token in text if token.isalpha()] \r\n",
        "\r\n",
        "    text = \" \".join(text)\r\n",
        "    return text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnWBCIRwKr_X"
      },
      "source": [
        "url = 'http://www.gutenberg.org/files/11/11-0.txt'"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgvICZOtnV9N"
      },
      "source": [
        "text = requests.get(url)\r\n",
        "\r\n",
        "text = re.split(r'Down the Rabbit-Hole', text.text)[2]\r\n",
        "text = re.split(r'THE END', text)[0]\r\n",
        "\r\n",
        "text = re.split(r'CHAPTER', text)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUOeJGTguUQv"
      },
      "source": [
        "text_wo_alice = []\r\n",
        "stop_words = stopwords.words(\"english\")\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "\r\n",
        "for chapter in text:\r\n",
        "  chapter = clean_text(chapter)\r\n",
        "  text_wo_alice.append(chapter)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfvCWfhv8FJG"
      },
      "source": [
        "vectorizer=TfidfVectorizer(use_idf=True)\r\n",
        "tfIdf = vectorizer.fit_transform(text_wo_alice)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1bwvTtg8Jt7",
        "outputId": "01442a8d-dece-4928-d80d-fa976f70c402"
      },
      "source": [
        "words = vectorizer.get_feature_names()\r\n",
        "for i in range(len(text_wo_alice)):\r\n",
        "    result_list = []\r\n",
        "    for k in range(len(words)):\r\n",
        "        result_list.append([words[k], float(tfIdf[i].T.todense()[k])])\r\n",
        "        \r\n",
        "    print('The most 10 important words in chapter %d:'%(i+1))\r\n",
        "    print(', '.join(list(map(lambda el: el[0], \r\n",
        "                         sorted(result_list, key = lambda l: l[1], reverse = True)[:10]))))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The most 10 important words in chapter 1:\n",
            "little, eat, like, either, think, bottle, marked, one, rabbit, nothing\n",
            "The most 10 important words in chapter 2:\n",
            "mouse, little, pool, swam, cried, said, oh, like, must, cat\n",
            "The most 10 important words in chapter 3:\n",
            "said, dodo, mouse, dry, bird, old, soon, one, know, crowded\n",
            "The most 10 important words in chapter 4:\n",
            "little, puppy, rabbit, one, said, room, bill, fan, heard, grow\n",
            "The most 10 important words in chapter 5:\n",
            "said, caterpillar, pigeon, hookah, little, tried, one, green, father, side\n",
            "The most 10 important words in chapter 6:\n",
            "said, footman, baby, cat, duchess, like, little, cook, mad, know\n",
            "The most 10 important words in chapter 7:\n",
            "said, dormouse, march, hatter, hare, clock, thing, know, time, tea\n",
            "The most 10 important words in chapter 8:\n",
            "said, queen, soldier, three, hedgehog, king, cat, executioner, gardener, head\n",
            "The most 10 important words in chapter 9:\n",
            "mock, said, turtle, moral, gryphon, duchess, went, queen, never, old\n",
            "The most 10 important words in chapter 10:\n",
            "mock, turtle, said, gryphon, join, lobster, soo, beautiful, would, repeat\n",
            "The most 10 important words in chapter 11:\n",
            "said, king, hatter, dormouse, one, juror, officer, witness, white, court\n",
            "The most 10 important words in chapter 12:\n",
            "said, king, jury, would, white, gave, little, sister, lizard, nothing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJcMYVAQJycY"
      },
      "source": [
        "vectorizer=TfidfVectorizer(use_idf=False)\r\n",
        "tfIdf = vectorizer.fit_transform(text_wo_alice)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfRvFB9TJ2LB",
        "outputId": "cb099fb6-5b07-48b4-b8fc-279c6b881e7e"
      },
      "source": [
        "words = vectorizer.get_feature_names()\r\n",
        "for i in range(len(text_wo_alice)):\r\n",
        "    result_list = []\r\n",
        "    for k in range(len(words)):\r\n",
        "        result_list.append([words[k], float(tfIdf[i].T.todense()[k])])\r\n",
        "        \r\n",
        "    print('The most 10 important words in chapter %d:'%(i+1))\r\n",
        "    print(', '.join(list(map(lambda el: el[0], \r\n",
        "                         sorted(result_list, key = lambda l: l[1], reverse = True)[:10]))))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The most 10 important words in chapter 1:\n",
            "little, like, think, one, thought, eat, found, get, nothing, said\n",
            "The most 10 important words in chapter 2:\n",
            "little, mouse, said, like, must, cried, time, went, could, go\n",
            "The most 10 important words in chapter 3:\n",
            "said, mouse, dodo, one, dry, know, long, soon, would, bird\n",
            "The most 10 important words in chapter 4:\n",
            "little, said, one, rabbit, get, heard, came, quite, thought, go\n",
            "The most 10 important words in chapter 5:\n",
            "said, caterpillar, little, one, got, pigeon, tried, felt, good, like\n",
            "The most 10 important words in chapter 6:\n",
            "said, like, little, cat, could, much, went, would, get, know\n",
            "The most 10 important words in chapter 7:\n",
            "said, march, dormouse, hatter, hare, thing, time, know, went, little\n",
            "The most 10 important words in chapter 8:\n",
            "said, queen, three, head, one, went, came, like, king, looked\n",
            "The most 10 important words in chapter 9:\n",
            "said, mock, turtle, went, never, gryphon, duchess, little, make, queen\n",
            "The most 10 important words in chapter 10:\n",
            "said, mock, turtle, would, gryphon, join, lobster, beautiful, could, soo\n",
            "The most 10 important words in chapter 11:\n",
            "said, king, one, hatter, white, dormouse, next, queen, rabbit, thought\n",
            "The most 10 important words in chapter 12:\n",
            "said, would, king, little, white, gave, jury, could, head, know\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdmrUToLmn6Y"
      },
      "source": [
        "url = 'http://www.gutenberg.org/files/11/11-0.txt'\n",
        "text = requests.get(url)\n",
        "\n",
        "res = re.split(r'Down the Rabbit-Hole', text.text)[2]\n",
        "res = re.split(r'THE END', res)[0]\n",
        "\n",
        "res = re.sub(r'[\\x00-\\x1f\\x7f-\\x9fâ_]', '', res)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCoxxiy1mn6Z"
      },
      "source": [
        "split_regex = re.compile(r'[.|!|?|…]') # split the sentences by '.!&...' marks\n",
        "sentences = filter(lambda t: t, [t.strip() for t in split_regex.split(res)])\n",
        "for_Alice = list(sentences)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqaXe6iSmn6Z"
      },
      "source": [
        "sentences_w_alice = []\n",
        "for i in range(len(for_Alice)):\n",
        "    if for_Alice[i].find('Alice') != -1:\n",
        "        sentences_w_alice.append(for_Alice[i])\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqp8oreNmn6Z"
      },
      "source": [
        "final_text = []\n",
        "for sentence in sentences_w_alice:    \n",
        "  sentence = clean_text(sentence)\n",
        "  final_text.append(sentence)\n",
        "  \n",
        "final_text = ' '.join(final_text)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrfD9sVCmn6a"
      },
      "source": [
        "tokens = WhitespaceTokenizer().tokenize(final_text) "
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFgv-KMOmn6a"
      },
      "source": [
        "unique_tokens = []\n",
        "for token in tokens:\n",
        "    if token not in unique_tokens:\n",
        "        unique_tokens.append(token)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQCiEP59mn6a"
      },
      "source": [
        "number_of_unique_tokens = []\n",
        "for token in unique_tokens:\n",
        "    number_of_unique_tokens.append(tokens.count(token))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6H4mT9mmn6b"
      },
      "source": [
        "unique_tokens_ = []\n",
        "for i in range(len(unique_tokens)):\n",
        "    unique_tokens_.append([unique_tokens[i], number_of_unique_tokens[i]])\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df5iS5BNmn6b"
      },
      "source": [
        "sorted_unique_tokens = sorted(unique_tokens_, key = lambda i: i[1], reverse = True)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOX-M8ZGmn6c"
      },
      "source": [
        "for_pos_tag = []\n",
        "for i in sorted_unique_tokens:\n",
        "    for_pos_tag.append(i[0])"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8qnLeqQmn6c"
      },
      "source": [
        "tagged_sorted_unique_tokens = nltk.pos_tag(for_pos_tag)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYnMko8imn6c"
      },
      "source": [
        "final_verb = []\n",
        "for i in range(len(for_pos_tag)):\n",
        "    if tagged_sorted_unique_tokens[i][1] in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
        "        final_verb.append(for_pos_tag[i])"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OymOHcvhmn6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3809eec-9ed3-47b6-ba27-a93fb0f5b757"
      },
      "source": [
        "print('The most important verbs in sentences with Alice are the following:')\n",
        "print(', '.join(final_verb[:10]))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The most important verbs in sentences with Alice are the following:\n",
            "said, thought, went, looked, got, say, began, think, see, go\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}